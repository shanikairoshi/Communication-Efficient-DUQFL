{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee84f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc18ac0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: genomic-benchmarks in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (0.0.9)\n",
      "Requirement already satisfied: biopython>=1.79 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from genomic-benchmarks) (1.85)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from genomic-benchmarks) (2.32.3)\n",
      "Requirement already satisfied: pip>=20.0.1 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from genomic-benchmarks) (24.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from genomic-benchmarks) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from genomic-benchmarks) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from genomic-benchmarks) (4.66.5)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from genomic-benchmarks) (6.0.1)\n",
      "Requirement already satisfied: gdown>=4.2.0 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from genomic-benchmarks) (5.2.0)\n",
      "Requirement already satisfied: yarl in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from genomic-benchmarks) (1.11.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from gdown>=4.2.0->genomic-benchmarks) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from gdown>=4.2.0->genomic-benchmarks) (3.13.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->genomic-benchmarks) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->genomic-benchmarks) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->genomic-benchmarks) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.23.0->genomic-benchmarks) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.23.0->genomic-benchmarks) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.23.0->genomic-benchmarks) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from requests>=2.23.0->genomic-benchmarks) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm>=4.41.1->genomic-benchmarks) (0.4.6)\n",
      "Requirement already satisfied: multidict>=4.0 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from yarl->genomic-benchmarks) (6.0.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1.4->genomic-benchmarks) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown>=4.2.0->genomic-benchmarks) (2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from requests[socks]->gdown>=4.2.0->genomic-benchmarks) (1.7.1)\n",
      "Requirement already satisfied: qiskit in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: qiskit_machine_learning in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (0.8.2)\n",
      "Requirement already satisfied: qiskit_algorithms in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: qiskit-aer in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (0.17.0)\n",
      "Requirement already satisfied: rustworkx>=0.15.0 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from qiskit) (0.16.0)\n",
      "Requirement already satisfied: numpy<3,>=1.17 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from qiskit) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from qiskit) (1.13.1)\n",
      "Requirement already satisfied: sympy>=1.3 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from qiskit) (1.14.0)\n",
      "Requirement already satisfied: dill>=0.3 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from qiskit) (0.3.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from qiskit) (2.9.0.post0)\n",
      "Requirement already satisfied: stevedore>=3.0.0 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from qiskit) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from qiskit) (4.11.0)\n",
      "Requirement already satisfied: symengine<0.14,>=0.11 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from qiskit) (0.13.0)\n",
      "Requirement already satisfied: psutil>=5 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from qiskit_machine_learning) (5.9.0)\n",
      "Requirement already satisfied: scikit-learn>=1.2 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from qiskit_machine_learning) (1.5.1)\n",
      "Requirement already satisfied: setuptools>=40.1 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from qiskit_machine_learning) (75.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.0->qiskit) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn>=1.2->qiskit_machine_learning) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn>=1.2->qiskit_machine_learning) (3.5.0)\n",
      "Requirement already satisfied: pbr>=2.0.0 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from stevedore>=3.0.0->qiskit) (6.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\s222112938\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy>=1.3->qiskit) (1.3.0)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "!pip install genomic-benchmarks\n",
    "!pip install qiskit qiskit_machine_learning qiskit_algorithms qiskit-aer\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "PROJ = Path.cwd() / \"tDuqfl_Project\"\n",
    "if str(PROJ) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "PROJ = Path.cwd() / \"tDuQFL_Project\"\n",
    "if str(PROJ) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJ))\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Teleportation/tDuQFL_Project')\n",
    "# ─── 5. Assemble filenames for each artifact ─────────────────────────────────\n",
    "drive_root = \"/content/drive/MyDrive/Teleportation/tDuQFL_Project/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73b834fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "PROJ = Path.cwd() / \"tDuQFL_Project\"\n",
    "if str(PROJ) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c245a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.imports import *\n",
    "from configs.dataset_genome_iid import *     # swap to other configs as needed\n",
    "from io_utils.naming import stamp_now, flags, build_param_str, make_filenames\n",
    "\n",
    "start_str, date_str = stamp_now()\n",
    "teleport_pl, noise_pl = flags(use_teleportation, use_noise)\n",
    "param_str = build_param_str(num_clients, num_federated_layers, num_deep_unfolding_iterations,\n",
    "                            initial_learning_rate, initial_perturbation)\n",
    "\n",
    "best_client_csv_file, global_csv_file, local_csv_file, validation_csv_file = make_filenames(\n",
    "    drive_root, dataset_name, split_type, date_str, teleport_pl, noise_pl, param_str\n",
    ")\n",
    "from io_utils.csv_logger import init_local_csv, init_best_csv, init_validation_csv\n",
    "\n",
    "# Create folders + write headers\n",
    "init_best_csv(best_client_csv_file)\n",
    "\n",
    "local_headers = [\n",
    "    \"Federated Round\", \"Client Number\", \"Iteration\",\n",
    "    \"Objective Function Value\", \"Training Accuracy\", \"Test Accuracy\",\n",
    "    \"Learning Rate\", \"Perturbation\"\n",
    "]\n",
    "init_local_csv(local_csv_file, local_headers)\n",
    "\n",
    "init_validation_csv(validation_csv_file)\n",
    "\n",
    "# Do NOT pre-init global_csv_file here because your save_accuracies_to_csv()\n",
    "# already writes the header each time it runs (in 'w' mode)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324178e0",
   "metadata": {},
   "source": [
    "Load and Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1407919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.preprocess_mnist import load_and_prepare_dataset\n",
    "from data.splitters import split_dataset_for_epochs  # or your non-IID splitter\n",
    "from configs.base_config import (\n",
    "    num_clients, num_epochs, samples_per_epoch, split_type,\n",
    "    global_seed\n",
    ")\n",
    "\n",
    "# choose digits and target dimensionality for the quantum model\n",
    "target_features = 8          # e.g., 8–16 works well to start\n",
    "digit_a, digit_b = 3, 8      # binary task (keeps your current model unchanged)\n",
    "\n",
    "np_train_data, np_test_data = load_and_prepare_dataset(\n",
    "    n_features=target_features,\n",
    "    digit_a=digit_a,\n",
    "    digit_b=digit_b,\n",
    "    global_seed=global_seed\n",
    ")\n",
    "\n",
    "# Capacity guard so epoch indexing never overflows\n",
    "N_train = len(np_train_data)\n",
    "train_capacity = N_train // (num_clients * samples_per_epoch)\n",
    "num_epochs = min(num_epochs, train_capacity)  # <- do this right after loading\n",
    "\n",
    "# Build clients (IID)\n",
    "clients = split_dataset_for_epochs(\n",
    "    num_clients=num_clients,\n",
    "    num_epochs=num_epochs,\n",
    "    train_data=np_train_data,\n",
    "    test_data=np_test_data,\n",
    "    samples_per_epoch=samples_per_epoch\n",
    ")\n",
    "\n",
    "# Or Non-IID:\n",
    "# from data.noniid_mnist import split_dataset_quantity_non_iid_binary\n",
    "# clients_data = split_dataset_quantity_non_iid_binary(np_train_data, num_clients, num_epochs, samples_per_epoch,\n",
    "#                                                      non_iid_ratio=0.8, quantity_variation=0.3)\n",
    "# clients = [Client(cd, test_data=np_test_data) for cd in clients_data]\n",
    "\n",
    "# Validation/table arrays (unchanged)\n",
    "test_sequences = np.array([d[\"sequence\"] for d in np_test_data])\n",
    "test_labels    = np.array([d[\"label\"]    for d in np_test_data])\n",
    "X_val, y_val   = test_sequences, test_labels\n",
    "\n",
    "# num_features is inferred by your training loop from the first client's sequence.\n",
    "# With the MNIST preprocessor, len(sequence) == target_features, so nothing else to change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d512e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from data.preprocess_genome import load_and_prepare_dataset\n",
    "from data.splitters import split_dataset_for_epochs\n",
    "from configs.base_config import (\n",
    "    num_clients, num_epochs, samples_per_epoch, split_type,\n",
    "    global_seed\n",
    ")\n",
    "\n",
    "np_train_data, np_test_data = load_and_prepare_dataset(word_size, global_seed)\n",
    "\n",
    "# 2) Compute feasible epoch capacity and cap both epochs and rounds\n",
    "N_train = len(np_train_data)\n",
    "train_capacity = N_train // (num_clients * samples_per_epoch)\n",
    "num_epochs_eff = max(1, min(num_epochs, train_capacity))\n",
    "\n",
    "if train_capacity == 0:\n",
    "    raise ValueError(\n",
    "        f\"Not enough training samples ({N_train}) for \"\n",
    "        f\"{num_clients=} × {samples_per_epoch=} per epoch. \"\n",
    "        \"Reduce samples_per_epoch or num_clients, or enable resampling.\"\n",
    "    )\n",
    "\n",
    "num_federated_layers_eff = min(num_federated_layers, num_epochs_eff)\n",
    "\n",
    "# Build clients\n",
    "if split_type.lower() == \"iid\":\n",
    "    from data.splitters import split_dataset_for_epochs\n",
    "    clients = split_dataset_for_epochs(\n",
    "        num_clients=num_clients,\n",
    "        num_epochs=num_epochs_eff,             # or num_epochs\n",
    "        train_data=np_train_data,\n",
    "        test_data=np_test_data,\n",
    "        samples_per_epoch=samples_per_epoch,\n",
    "    )\n",
    "elif split_type.lower() in {\"noniid\", \"non-iid\", \"non_iid\"}:\n",
    "    from data.noniid import make_non_iid_clients\n",
    "    clients = make_non_iid_clients(\n",
    "        train_data=np_train_data,\n",
    "        test_data=np_test_data,\n",
    "        num_clients=num_clients,\n",
    "        num_epochs=num_epochs_eff,             # or num_epochs\n",
    "        samples_per_epoch=samples_per_epoch,\n",
    "        non_iid_ratio=0.8,                     # tune as needed\n",
    "        quantity_variation=0.5,                # tune as needed\n",
    "        seed=global_seed,\n",
    "        plot=True\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Unknown split_type: {split_type}\")\n",
    "\n",
    "\n",
    "clients = split_dataset_for_epochs(\n",
    "    num_clients=num_clients, num_epochs=num_epochs,\n",
    "    train_data=np_train_data, test_data=np_test_data,\n",
    "    samples_per_epoch=samples_per_epoch\n",
    ")\n",
    "'''\n",
    "# validation/tables\n",
    "test_sequences = np.array([d[\"sequence\"] for d in np_test_data])\n",
    "test_labels    = np.array([d[\"label\"]    for d in np_test_data])\n",
    "X_val, y_val   = test_sequences, test_labels\n",
    "\n",
    "# derive num_features once\n",
    "if clients and clients[0].data and clients[0].data[0]:\n",
    "    num_features = clients[0].data[0][0]['sequence'].shape[0]\n",
    "else:\n",
    "    raise RuntimeError(\"Empty client data – check splitting indices.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7dd9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer num_features from the first available sample in clients\n",
    "def infer_num_features_from_clients(clients):\n",
    "    for c in clients:\n",
    "        for epoch_data in c.data:              # list of samples for that epoch\n",
    "            if not epoch_data:\n",
    "                continue\n",
    "            sample = epoch_data[0]\n",
    "            if \"sequence\" in sample:           # your Genome pipeline\n",
    "                arr = np.asarray(sample[\"sequence\"])\n",
    "                return int(arr.size)\n",
    "            if \"features\" in sample:           # some other pipelines\n",
    "                arr = np.asarray(sample[\"features\"])\n",
    "                return int(arr.size)\n",
    "            if \"image\" in sample:              # e.g., MNIST before flatten\n",
    "                arr = np.asarray(sample[\"image\"]).reshape(-1)\n",
    "                return int(arr.size)\n",
    "            # add any other key you use\n",
    "    raise RuntimeError(\"Could not infer num_features: no samples found.\")\n",
    "\n",
    "num_features = infer_num_features_from_clients(clients)\n",
    "print(f\"[info] num_features = {num_features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d847bd00",
   "metadata": {},
   "source": [
    "run federated loop and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc26297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.loop import run_federated_training\n",
    "from configs.base_config import use_teleportation as CFG_TEL, noise_preset, shots_used\n",
    "from training.metrics import metrics_init, metrics_log_round, metrics_finalize, compute_auc,metrics_summarize\n",
    "from viz.plots import plot_accuracy_curve, plot_val_loss, plot_time_per_round, plot_fidelity_vs_delta_acc, plot_beta_hist, plot_client_fairness_last_round\n",
    "# Initialize metrics store once\n",
    "metrics_store = metrics_init(\n",
    "    log_path=os.path.join(drive_root, \"teleport_metrics_Perturb_shrink.csv\")\n",
    ")\n",
    "\n",
    "global_acc, clients_train, clients_test, round_times, val_losses, info_last = run_federated_training(\n",
    "    clients=clients,\n",
    "    num_federated_layers=num_federated_layers,\n",
    "    num_deep_unfolding_iterations=num_deep_unfolding_iterations,\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    initial_perturbation=initial_perturbation,\n",
    "    num_features=num_features,\n",
    "    best_client_csv_file=best_client_csv_file,\n",
    "    global_csv_file=global_csv_file,\n",
    "    local_csv_file=local_csv_file,\n",
    "    validation_csv_file=validation_csv_file,\n",
    "    test_sequences=test_sequences,\n",
    "    test_labels=test_labels,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    use_teleportation=CFG_TEL,          # ← important\n",
    "    noise_preset=noise_preset,\n",
    "    shots_used=shots_used,\n",
    "    metrics=metrics_store,   # <-- pass it in\n",
    ")\n",
    "\n",
    "rows_np = metrics_finalize(metrics_store)   # if you need the in-memory array\n",
    "#summary = metrics_summarize(metrics_store)  # prints a concise summary, returns a dict\n",
    "\n",
    "# quick visuals\n",
    "rounds = list(range(len(global_acc)))\n",
    "plot_accuracy_curve(rounds, global_acc, label=\"Global accuracy (DT-DUQFL)\")\n",
    "plot_val_loss(rounds, val_losses, label=\"Central validation loss\")\n",
    "plot_time_per_round(rounds, round_times)\n",
    "\n",
    "if info_last is not None:\n",
    "    # this uses \"last\" round's info; in your logger you kept per-round arrays; adapt if needed\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
